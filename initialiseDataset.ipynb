{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22dc9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tables...\n",
      "Cleaning data...\n",
      "Merging tables...\n",
      "Further cleaning data...\n",
      "Fetching plot summaries and posters...\n",
      "Batch 1/63 completed\n",
      "Batch 2/63 completed\n",
      "Batch 3/63 completed\n",
      "Batch 4/63 completed\n",
      "Batch 5/63 completed\n",
      "Batch 6/63 completed\n",
      "Batch 7/63 completed\n",
      "Batch 8/63 completed\n",
      "Batch 9/63 completed\n",
      "Batch 10/63 completed\n",
      "Batch 11/63 completed\n",
      "Batch 12/63 completed\n",
      "Batch 13/63 completed\n",
      "Batch 14/63 completed\n",
      "Batch 15/63 completed\n",
      "Batch 16/63 completed\n",
      "Batch 17/63 completed\n",
      "Batch 18/63 completed\n",
      "Batch 19/63 completed\n",
      "Batch 20/63 completed\n",
      "Batch 21/63 completed\n",
      "Batch 22/63 completed\n",
      "Batch 23/63 completed\n",
      "Batch 24/63 completed\n",
      "Batch 25/63 completed\n",
      "Batch 26/63 completed\n",
      "Batch 27/63 completed\n",
      "Batch 28/63 completed\n",
      "Batch 29/63 completed\n",
      "Batch 30/63 completed\n",
      "Batch 31/63 completed\n",
      "Batch 32/63 completed\n",
      "Batch 33/63 completed\n",
      "Batch 34/63 completed\n",
      "Batch 35/63 completed\n",
      "Batch 36/63 completed\n",
      "Batch 37/63 completed\n",
      "Batch 38/63 completed\n",
      "Batch 39/63 completed\n",
      "Batch 40/63 completed\n",
      "Batch 41/63 completed\n",
      "Batch 42/63 completed\n",
      "Batch 43/63 completed\n",
      "Batch 44/63 completed\n",
      "Batch 45/63 completed\n",
      "Batch 46/63 completed\n",
      "Batch 47/63 completed\n",
      "Batch 48/63 completed\n",
      "Batch 49/63 completed\n",
      "Batch 50/63 completed\n",
      "Batch 51/63 completed\n",
      "Batch 52/63 completed\n",
      "Batch 53/63 completed\n",
      "Batch 54/63 completed\n",
      "Batch 55/63 completed\n",
      "Batch 56/63 completed\n",
      "Batch 57/63 completed\n",
      "Batch 58/63 completed\n",
      "Batch 59/63 completed\n",
      "Batch 60/63 completed\n",
      "Batch 61/63 completed\n",
      "Batch 62/63 completed\n",
      "Batch 63/63 completed\n",
      "Exporting to json...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 177\u001b[0m\n\u001b[1;32m    174\u001b[0m film_data \u001b[38;5;241m=\u001b[39m film_data\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    175\u001b[0m result \u001b[38;5;241m=\u001b[39m film_data\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebpage/films.json\u001b[39m\u001b[38;5;124m'\u001b[39m ,orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal films: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(film_data))\n\u001b[1;32m    179\u001b[0m exit(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "#import modules, packages and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as req\n",
    "import gzip\n",
    "import concurrent.futures\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from tmdb_calls import doBatch\n",
    "from multiprocessing import Manager\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "print('Downloading tables...')\n",
    "\n",
    "#get film datasets\n",
    "\n",
    "#set urls\n",
    "url_title_basics = 'https://datasets.imdbws.com/title.basics.tsv.gz' #film name, year, runtime, genres\n",
    "url_crew = 'https://datasets.imdbws.com/title.principals.tsv.gz' #actors, actresses, cinematographers, directors (redundant)\n",
    "url_ratings = 'https://datasets.imdbws.com/title.ratings.tsv.gz' #ratings for films (not all)\n",
    "url_names = 'https://datasets.imdbws.com/name.basics.tsv.gz' #link table for names against nconst\n",
    "url_langs = 'https://datasets.imdbws.com/title.akas.tsv.gz' #link table for names against nconst\n",
    "\n",
    "#download from url\n",
    "res_title_basics = req.get(url_title_basics).content\n",
    "res_crew = req.get(url_crew).content\n",
    "res_ratings = req.get(url_ratings).content\n",
    "res_names = req.get(url_names).content\n",
    "res_lang = req.get(url_langs).content\n",
    "\n",
    "#decompress\n",
    "title_basics_gzip = gzip.decompress(res_title_basics)\n",
    "crew_basics_gzip = gzip.decompress(res_crew)\n",
    "title_ratings_gzip = gzip.decompress(res_ratings)\n",
    "names_gzip = gzip.decompress(res_names)\n",
    "title_langs_gzip = gzip.decompress(res_lang)\n",
    "\n",
    "#read csv into dataframes\n",
    "titles = pd.read_csv(BytesIO(title_basics_gzip), delimiter='\\t',low_memory=False)\n",
    "crew = pd.read_csv(BytesIO(crew_basics_gzip), delimiter='\\t',low_memory=False)\n",
    "ratings = pd.read_csv(BytesIO(title_ratings_gzip), delimiter='\\t',low_memory=False)\n",
    "names = pd.read_csv(BytesIO(names_gzip), delimiter='\\t',low_memory=False)\n",
    "langs = pd.read_csv(BytesIO(title_langs_gzip), delimiter='\\t',low_memory=False)\n",
    "\n",
    "\n",
    "print('Cleaning data...')\n",
    "\n",
    "#clean data\n",
    "\n",
    "# #filter only english films\n",
    "desired_langs = ['en']\n",
    "filtered_langs = langs[langs['language'].isin(desired_langs)]\n",
    "tconsts_filtered_langs = filtered_langs['titleId'].tolist()\n",
    "desired_regions = ['CA', 'US', 'GB', 'IE', 'AU', 'NZ']\n",
    "filtered_regions = langs[langs['region'].isin(desired_regions)]\n",
    "tconsts_filtered_regions = filtered_regions['titleId'].tolist()\n",
    "\n",
    "#remove unsuitable titles\n",
    "titles = titles[titles['titleType'] == 'movie']\n",
    "titles = titles[titles['genres'] != r'\\N']\n",
    "titles['isAdult'] = pd.to_numeric(titles['isAdult'], errors='coerce')\n",
    "titles = titles[titles['isAdult'] == 0 ]\n",
    "titles = titles[(titles['startYear'] >= '1955') & (titles['startYear'] != r'\\N')]\n",
    "titles = titles[(titles['tconst'].isin(tconsts_filtered_langs) & (titles['tconst'].isin(tconsts_filtered_regions)))]\n",
    "\n",
    "#get tconsts for remaining non-film rows, and remove corresponding non-film rows\n",
    "film_tconsts = titles['tconst'].tolist()\n",
    "crew = crew[crew['tconst'].isin(film_tconsts)]\n",
    "ratings = ratings[ratings['tconst'].isin(film_tconsts)]\n",
    "\n",
    "#set columns to remove from dataset\n",
    "remove_from_titles = ['originalTitle', 'endYear', 'titleType', 'isAdult']\n",
    "remove_from_crew = ['ordering','job','characters']\n",
    "remove_from_ratings = ['numVotes']\n",
    "remove_from_names = ['birthYear', 'deathYear', 'primaryProfession', 'knownForTitles']\n",
    "\n",
    "#remove unneeded columns\n",
    "titles = titles.drop(columns=remove_from_titles)\n",
    "crew = crew.drop(columns=remove_from_crew)\n",
    "ratings = ratings.drop(columns=remove_from_ratings)\n",
    "names = names.drop(columns=remove_from_names)\n",
    "\n",
    "\n",
    "print('Merging tables...')\n",
    "\n",
    "#merge relational tables\n",
    "\n",
    "crew_data = crew.copy()\n",
    "\n",
    "#merge crew data with names table to get respective names rather than nconst\n",
    "crew_data['nconst'] = crew_data['nconst'].str.split(', ')\n",
    "crew_data = crew_data.explode('nconst')\n",
    "crew_data = pd.merge(crew_data, names, on='nconst', how='left')\n",
    "crew_data = crew_data.pivot_table(\n",
    "    index=['tconst'],\n",
    "    columns=['category'],\n",
    "    values=['primaryName'],\n",
    "    aggfunc=lambda x: ', '.join(str(item) for item in x),\n",
    ").reset_index()\n",
    "\n",
    "#formaat and restructure columns\n",
    "crew_data.columns = [' '.join(col).strip() for col in crew_data.columns.values]\n",
    "crew_data.columns = ['tconst', 'actor', 'actress', 'archive_footage', 'archive_sound', 'cinematographer', 'composer', 'director', 'editor', 'producer', 'production_designer', 'self', 'writer']\n",
    "\n",
    "#merge datasets for one complete table\n",
    "crew_data = crew_data.drop(columns=['archive_footage','archive_sound','self', 'production_designer'])\n",
    "film_data = pd.merge(titles, ratings, on='tconst', how='left')\n",
    "film_data = pd.merge(film_data, crew_data, on='tconst', how='left')\n",
    "\n",
    "\n",
    "print('Further cleaning data...')\n",
    "\n",
    "#remove data-sparse films\n",
    "\n",
    "columns_check = ['director', 'cinematographer', 'editor', 'writer', 'composer', 'producer']\n",
    "film_data = film_data[film_data[columns_check].isna().sum(axis=1) < 4]\n",
    "\n",
    "film_data = film_data.dropna(subset=['actor', 'actress'])\n",
    "film_data = film_data.dropna(subset=['runtimeMinutes'])\n",
    "film_data = film_data.dropna(subset=['averageRating'])\n",
    "film_data = film_data.dropna(subset=['genres'])\n",
    "\n",
    "#add columns for plot and poster path\n",
    "film_data['plot'] = np.nan\n",
    "film_data['poster'] = np.nan\n",
    "\n",
    "\n",
    "print('Fetching plot summaries and posters...')\n",
    "\n",
    "#get film plot and poster with tmdb api ~ >2hrs\n",
    "\n",
    "\n",
    "#call api/details for each film with multiprocessing and mutlithreading\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    manager = Manager()\n",
    "    shared_data = manager.Namespace()\n",
    "    agg_list = []\n",
    "\n",
    "    batch_size = 1000\n",
    "    sleep_time = 3\n",
    "\n",
    "    num_batches = (len(film_data) // batch_size) + 1\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(8) as process_executor:\n",
    "\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            start_index = i * batch_size\n",
    "            end_index = (i + 1) * batch_size\n",
    "            \n",
    "            shared_data.film_data = film_data.iloc[start_index:end_index]\n",
    "\n",
    "            future = process_executor.submit(doBatch, shared_data)\n",
    "\n",
    "            concurrent.futures.wait([future])\n",
    "\n",
    "            agg_list.append(shared_data.film_data)\n",
    "\n",
    "            print(f\"Batch {i+1}/{num_batches} completed\")\n",
    "                \n",
    "    film_data = pd.concat(agg_list, ignore_index=True)\n",
    "\n",
    "#remove films with no plot\n",
    "film_data = film_data.dropna(subset=['plot'])\n",
    "\n",
    "print('Exporting to json...')\n",
    "\n",
    "\n",
    "#export film data to json\n",
    "\n",
    "#shuffle order\n",
    "film_data = film_data.sample(frac=1)\n",
    "result = film_data.to_json('webpage/films.json' ,orient=\"records\")\n",
    "\n",
    "print('Total films: ' + str(len(film_data)))\n",
    "\n",
    "exit(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
