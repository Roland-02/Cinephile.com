{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cc729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tables...\n",
      "Cleaning data...\n",
      "Merging tables...\n",
      "Further cleaning data...\n",
      "EDL testing...\n",
      "Films: 31107\n",
      "Fetching plot summaries and posters...\n",
      "Batch 1/32 completed\n",
      "Batch 2/32 completed\n",
      "Batch 3/32 completed\n",
      "Batch 4/32 completed\n",
      "Batch 5/32 completed\n",
      "Batch 6/32 completed\n",
      "Batch 7/32 completed\n",
      "Batch 8/32 completed\n",
      "Batch 9/32 completed\n",
      "Batch 10/32 completed\n",
      "Batch 11/32 completed\n",
      "Batch 12/32 completed\n",
      "Batch 13/32 completed\n",
      "Batch 14/32 completed\n",
      "Batch 15/32 completed\n",
      "Batch 16/32 completed\n",
      "Batch 17/32 completed\n",
      "Batch 18/32 completed\n",
      "Batch 19/32 completed\n",
      "Batch 20/32 completed\n",
      "Batch 21/32 completed\n",
      "Batch 22/32 completed\n",
      "Batch 23/32 completed\n",
      "Batch 24/32 completed\n",
      "Batch 25/32 completed\n",
      "Batch 26/32 completed\n",
      "Batch 27/32 completed\n",
      "Batch 28/32 completed\n",
      "Batch 29/32 completed\n",
      "Batch 30/32 completed\n",
      "Batch 31/32 completed\n",
      "Batch 32/32 completed\n",
      "Exporting to sql...\n"
     ]
    }
   ],
   "source": [
    " #import modules, packages and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as req\n",
    "import gzip\n",
    "import concurrent.futures\n",
    "import seaborn as sns\n",
    "from webpage.routes.tmdb_calls import doBatch\n",
    "from multiprocessing import Manager\n",
    "from io import BytesIO\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "from langdetect import detect\n",
    "\n",
    "# ensure no duplicate cast members\n",
    "def remove_duplicates(names):\n",
    "    if isinstance(names, str):\n",
    "        unique_names = set(name.strip() for name in names.split(','))\n",
    "        return ', '.join(unique_names)\n",
    "    else:\n",
    "        return names\n",
    "\n",
    "# langdetec check if film title is in english\n",
    "def is_english(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang == 'en'\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "#export film data to mysql\n",
    "def save_mySQL(data):\n",
    "\n",
    "    # MySQL connection configuration\n",
    "    mydb = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"Leicester69lol\",\n",
    "        database=\"users\"\n",
    "    )\n",
    "\n",
    "    # Cursor object to execute SQL queries\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    # Table name in the database\n",
    "    table_name = \"all_films\"\n",
    "\n",
    "    # Define the SQL query to delete all records from the table\n",
    "    delete_query = \"DELETE FROM {}\".format(table_name)\n",
    "\n",
    "    # Execute the delete query\n",
    "    mycursor.execute(delete_query)\n",
    "    mydb.commit()\n",
    "\n",
    "    engine = create_engine(\"mysql+mysqlconnector://root:Leicester69lol@localhost/users\")\n",
    "\n",
    "    data.to_sql('all_films', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "def INITIALISE_FILM_DATASET():\n",
    "\n",
    "    print('Downloading tables...')\n",
    "\n",
    "    #get film datasets ~ 10-30mins\n",
    "\n",
    "    #set urls\n",
    "    url_title_basics = 'https://datasets.imdbws.com/title.basics.tsv.gz' #film name, year, runtime, genres\n",
    "    url_crew = 'https://datasets.imdbws.com/title.principals.tsv.gz' #actors, actresses, cinematographers, directors (redundant)\n",
    "    url_ratings = 'https://datasets.imdbws.com/title.ratings.tsv.gz' #ratings for films (not all)\n",
    "    url_names = 'https://datasets.imdbws.com/name.basics.tsv.gz' #link table for names against nconst\n",
    "    url_langs = 'https://datasets.imdbws.com/title.akas.tsv.gz' #link table for names against nconst\n",
    "\n",
    "    #download from url\n",
    "    res_title_basics = req.get(url_title_basics).content\n",
    "    res_crew = req.get(url_crew).content\n",
    "    res_ratings = req.get(url_ratings).content\n",
    "    res_names = req.get(url_names).content\n",
    "    res_lang = req.get(url_langs).content\n",
    "\n",
    "    #decompress\n",
    "    title_basics_gzip = gzip.decompress(res_title_basics)\n",
    "    crew_basics_gzip = gzip.decompress(res_crew)\n",
    "    title_ratings_gzip = gzip.decompress(res_ratings)\n",
    "    names_gzip = gzip.decompress(res_names)\n",
    "    title_langs_gzip = gzip.decompress(res_lang)\n",
    "\n",
    "    #read csv into dataframes\n",
    "    titles = pd.read_csv(BytesIO(title_basics_gzip), delimiter='\\t',low_memory=False)\n",
    "    crew = pd.read_csv(BytesIO(crew_basics_gzip), delimiter='\\t',low_memory=False)\n",
    "    ratings = pd.read_csv(BytesIO(title_ratings_gzip), delimiter='\\t',low_memory=False)\n",
    "    names = pd.read_csv(BytesIO(names_gzip), delimiter='\\t',low_memory=False)\n",
    "    langs = pd.read_csv(BytesIO(title_langs_gzip), delimiter='\\t',low_memory=False)\n",
    "\n",
    "\n",
    "    print('Cleaning data...')\n",
    "\n",
    "    #first data clean\n",
    "\n",
    "    # #filter only english films\n",
    "    desired_langs = ['en']\n",
    "    filtered_langs = langs[langs['language'].isin(desired_langs)]\n",
    "    tconsts_filtered_langs = filtered_langs['titleId'].tolist()\n",
    "    desired_regions = ['CA', 'US', 'GB', 'IE', 'AU', 'NZ']\n",
    "    filtered_regions = langs[langs['region'].isin(desired_regions)]\n",
    "    tconsts_filtered_regions = filtered_regions['titleId'].tolist()\n",
    "\n",
    "    #remove unsuitable titles\n",
    "    titles = titles[titles['titleType'] == 'movie']\n",
    "    titles = titles[titles['genres'] != r'\\N']\n",
    "    titles['isAdult'] = pd.to_numeric(titles['isAdult'], errors='coerce')\n",
    "    titles = titles[titles['isAdult'] == 0 ]\n",
    "    titles = titles[(titles['startYear'] >= '1955') & (titles['startYear'] != '\\\\N')]\n",
    "    titles = titles[(titles['tconst'].isin(tconsts_filtered_langs) & (titles['tconst'].isin(tconsts_filtered_regions)))]\n",
    "\n",
    "    #get tconsts for remaining non-film rows, and remove corresponding non-film rows\n",
    "    film_tconsts = titles['tconst'].tolist()\n",
    "    crew = crew[crew['tconst'].isin(film_tconsts)]\n",
    "    ratings = ratings[ratings['tconst'].isin(film_tconsts)]\n",
    "\n",
    "    #set columns to remove from dataset\n",
    "    remove_from_titles = ['originalTitle', 'endYear', 'titleType', 'isAdult']\n",
    "    remove_from_crew = ['ordering','job','characters']\n",
    "    remove_from_ratings = ['numVotes']\n",
    "    remove_from_names = ['birthYear', 'deathYear', 'primaryProfession', 'knownForTitles']\n",
    "\n",
    "    #remove unneeded columns\n",
    "    titles = titles.drop(columns=remove_from_titles)\n",
    "    crew = crew.drop(columns=remove_from_crew)\n",
    "    ratings = ratings.drop(columns=remove_from_ratings)\n",
    "    names = names.drop(columns=remove_from_names)\n",
    "\n",
    "    print('Merging tables...')\n",
    "\n",
    "    #merge relational tables\n",
    "\n",
    "    crew_data = crew.copy()\n",
    "\n",
    "    #merge crew data with names table to get respective names rather than nconst\n",
    "    crew_data['nconst'] = crew_data['nconst'].str.split(', ')\n",
    "    crew_data = crew_data.explode('nconst')\n",
    "    crew_data = pd.merge(crew_data, names, on='nconst', how='left')\n",
    "    crew_data = crew_data.pivot_table(\n",
    "        index=['tconst'],\n",
    "        columns=['category'],\n",
    "        values=['primaryName'],\n",
    "        aggfunc=lambda x: ', '.join(str(item) for item in x),\n",
    "    ).reset_index()\n",
    "\n",
    "\n",
    "    #format and restructure columns before merging\n",
    "    crew_data.columns = ['_'.join(col).strip() for col in crew_data.columns.values]\n",
    "    crew_data.columns = [col.replace('primaryName_', '') for col in crew_data.columns]\n",
    "    crew_data = crew_data.rename(columns={'tconst_': 'tconst'})\n",
    "    columns_to_keep = ['tconst', 'actor', 'actress', 'cinematographer', 'composer', 'director', 'editor', 'producer', 'writer']\n",
    "    crew_data = crew_data[columns_to_keep]\n",
    "\n",
    "    #merge film and cast datasets for one complete table\n",
    "    film_data = pd.merge(titles, ratings, on='tconst', how='left')\n",
    "    film_data = pd.merge(film_data, crew_data, on='tconst', how='left')\n",
    "\n",
    "    print('Further cleaning data...')\n",
    "\n",
    "    # second data clean, drop data sparse rows\n",
    "\n",
    "    columns_check = ['director', 'cinematographer', 'editor', 'writer', 'composer', 'producer']\n",
    "    film_data = film_data[film_data[columns_check].isna().sum(axis=1) == 0] #don't allow films with any missing data\n",
    "    film_data = film_data.dropna(subset=['actor', 'actress', 'runtimeMinutes', 'averageRating', 'genres'])\n",
    "\n",
    "    # double-check for null columns\n",
    "    film_data = film_data[film_data['runtimeMinutes'] != '\\\\N']\n",
    "    film_data = film_data[film_data['startYear'] != '\\\\N']\n",
    "    film_data = film_data[film_data['averageRating'] != '\\\\N']\n",
    "\n",
    "    # combine actor and actress into 1 column ~ 10 cast member (can reduce)\n",
    "    film_data['cast'] = film_data['actor'] + ', ' + film_data['actress']\n",
    "    film_data.drop(['actor', 'actress'], axis=1, inplace=True)\n",
    "    film_data['cast'] = film_data['cast'].apply(remove_duplicates) # double-check for duplicate cast members from merging\n",
    "\n",
    "    # remove data-sparse films\n",
    "    print('EDL testing...')\n",
    "\n",
    "    # check titles are in english (filter out MFL films release in West or mis-labelled)\n",
    "    english_titles = film_data['primaryTitle'].apply(is_english)\n",
    "    film_data = film_data[english_titles]\n",
    "\n",
    "    #add columns for plot and poster path\n",
    "    film_data['plot'] = np.nan\n",
    "    film_data['poster'] = np.nan\n",
    "\n",
    "\n",
    "    print('Films: ' + str(len(film_data)))\n",
    "\n",
    "\n",
    "    print('Fetching plot summaries and posters...')\n",
    "\n",
    "    # get film plot and poster with tmdb api ~ inconsistent runtime (<2Hrs)\n",
    "\n",
    "    #call api/details for each film with multiprocessing and mutlithreading\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        manager = Manager()\n",
    "        shared_data = manager.Namespace() #allow data to be shared with external function\n",
    "        agg_list = []\n",
    "\n",
    "        batch_size = 1000\n",
    "        sleep_time = 3\n",
    "\n",
    "        num_batches = (len(film_data) // batch_size) + 1\n",
    "\n",
    "        with concurrent.futures.ProcessPoolExecutor(8) as process_executor:\n",
    "\n",
    "            for i in range(num_batches):\n",
    "\n",
    "                start_index = i * batch_size\n",
    "                end_index = (i + 1) * batch_size\n",
    "                \n",
    "                shared_data.film_data = film_data.iloc[start_index:end_index]\n",
    "\n",
    "                future = process_executor.submit(doBatch, shared_data)\n",
    "\n",
    "                concurrent.futures.wait([future])\n",
    "\n",
    "                agg_list.append(shared_data.film_data)\n",
    "\n",
    "                print(f\"Batch {i+1}/{num_batches} completed\")\n",
    "                    \n",
    "        film_data = pd.concat(agg_list, ignore_index=True)\n",
    "\n",
    "    # #remove films with no plot\n",
    "    film_data = film_data.dropna(subset=['plot', 'poster'])\n",
    "\n",
    "    final_order = ['tconst','primaryTitle', 'plot', 'averageRating', 'genres', 'runtimeMinutes', 'startYear', 'cast', 'director', 'cinematographer', 'writer', 'producer', 'editor', 'composer', 'poster']\n",
    "    film_data = film_data[final_order]\n",
    "\n",
    "    print('Exporting to sql...')\n",
    "\n",
    "    #shuffle order\n",
    "    film_data = film_data.sample(frac=1)\n",
    "\n",
    "    # export film data to sql db\n",
    "    save_mySQL(film_data)\n",
    "\n",
    "    print('Final film count: '+str(len(film_data)))\n",
    "\n",
    "INITIALISE_FILM_DATASET()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
