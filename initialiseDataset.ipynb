{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21cc729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tables...\n",
      "Cleaning data...\n",
      "Merging tables...\n",
      "Further cleaning data...\n",
      "Films: 25527\n",
      "Fetching plot summaries and posters...\n",
      "Batch 1/26 completed\n",
      "Batch 2/26 completed\n",
      "Batch 3/26 completed\n",
      "Batch 4/26 completed\n",
      "Batch 5/26 completed\n",
      "Batch 6/26 completed\n",
      "Batch 7/26 completed\n",
      "Batch 8/26 completed\n",
      "Batch 9/26 completed\n",
      "Batch 10/26 completed\n",
      "Batch 11/26 completed\n",
      "Batch 12/26 completed\n",
      "Batch 13/26 completed\n",
      "Batch 14/26 completed\n",
      "Batch 15/26 completed\n",
      "Batch 16/26 completed\n",
      "Batch 17/26 completed\n",
      "Batch 18/26 completed\n",
      "Batch 19/26 completed\n",
      "Batch 20/26 completed\n",
      "Batch 21/26 completed\n",
      "Error in ThreadPoolExecutor: HTTPSConnectionPool(host='api.themoviedb.org', port=443): Max retries exceeded with url: /3/movie/tt3960584 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x2a5bf3d10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Batch 22/26 completed\n",
      "Batch 23/26 completed\n",
      "Batch 24/26 completed\n",
      "Batch 25/26 completed\n",
      "Batch 26/26 completed\n",
      "Exporting to json...\n",
      "Total films: 23643\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    " #import modules, packages and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as req\n",
    "import gzip\n",
    "import concurrent.futures\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from tmdb_calls import doBatch\n",
    "from multiprocessing import Manager\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "print('Downloading tables...')\n",
    "\n",
    "#get film datasets\n",
    "\n",
    "#set urls\n",
    "url_title_basics = 'https://datasets.imdbws.com/title.basics.tsv.gz' #film name, year, runtime, genres\n",
    "url_crew = 'https://datasets.imdbws.com/title.principals.tsv.gz' #actors, actresses, cinematographers, directors (redundant)\n",
    "url_ratings = 'https://datasets.imdbws.com/title.ratings.tsv.gz' #ratings for films (not all)\n",
    "url_names = 'https://datasets.imdbws.com/name.basics.tsv.gz' #link table for names against nconst\n",
    "url_langs = 'https://datasets.imdbws.com/title.akas.tsv.gz' #link table for names against nconst\n",
    "\n",
    "#download from url\n",
    "res_title_basics = req.get(url_title_basics).content\n",
    "res_crew = req.get(url_crew).content\n",
    "res_ratings = req.get(url_ratings).content\n",
    "res_names = req.get(url_names).content\n",
    "res_lang = req.get(url_langs).content\n",
    "\n",
    "#decompress\n",
    "title_basics_gzip = gzip.decompress(res_title_basics)\n",
    "crew_basics_gzip = gzip.decompress(res_crew)\n",
    "title_ratings_gzip = gzip.decompress(res_ratings)\n",
    "names_gzip = gzip.decompress(res_names)\n",
    "title_langs_gzip = gzip.decompress(res_lang)\n",
    "\n",
    "#read csv into dataframes\n",
    "titles = pd.read_csv(BytesIO(title_basics_gzip), delimiter='\\t',low_memory=False)\n",
    "crew = pd.read_csv(BytesIO(crew_basics_gzip), delimiter='\\t',low_memory=False)\n",
    "ratings = pd.read_csv(BytesIO(title_ratings_gzip), delimiter='\\t',low_memory=False)\n",
    "names = pd.read_csv(BytesIO(names_gzip), delimiter='\\t',low_memory=False)\n",
    "langs = pd.read_csv(BytesIO(title_langs_gzip), delimiter='\\t',low_memory=False)\n",
    "\n",
    "\n",
    "print('Cleaning data...')\n",
    "\n",
    "#clean data\n",
    "\n",
    "# #filter only english films\n",
    "desired_langs = ['en']\n",
    "filtered_langs = langs[langs['language'].isin(desired_langs)]\n",
    "tconsts_filtered_langs = filtered_langs['titleId'].tolist()\n",
    "desired_regions = ['CA', 'US', 'GB', 'IE', 'AU', 'NZ']\n",
    "filtered_regions = langs[langs['region'].isin(desired_regions)]\n",
    "tconsts_filtered_regions = filtered_regions['titleId'].tolist()\n",
    "\n",
    "#remove unsuitable titles\n",
    "titles = titles[titles['titleType'] == 'movie']\n",
    "titles = titles[titles['genres'] != r'\\N']\n",
    "titles['isAdult'] = pd.to_numeric(titles['isAdult'], errors='coerce')\n",
    "titles = titles[titles['isAdult'] == 0 ]\n",
    "titles = titles[(titles['startYear'] >= '1955') & (titles['startYear'] != '\\\\N')]\n",
    "titles = titles[(titles['tconst'].isin(tconsts_filtered_langs) & (titles['tconst'].isin(tconsts_filtered_regions)))]\n",
    "\n",
    "#get tconsts for remaining non-film rows, and remove corresponding non-film rows\n",
    "film_tconsts = titles['tconst'].tolist()\n",
    "crew = crew[crew['tconst'].isin(film_tconsts)]\n",
    "ratings = ratings[ratings['tconst'].isin(film_tconsts)]\n",
    "\n",
    "#set columns to remove from dataset\n",
    "remove_from_titles = ['originalTitle', 'endYear', 'titleType', 'isAdult']\n",
    "remove_from_crew = ['ordering','job','characters']\n",
    "remove_from_ratings = ['numVotes']\n",
    "remove_from_names = ['birthYear', 'deathYear', 'primaryProfession', 'knownForTitles']\n",
    "\n",
    "#remove unneeded columns\n",
    "titles = titles.drop(columns=remove_from_titles)\n",
    "crew = crew.drop(columns=remove_from_crew)\n",
    "ratings = ratings.drop(columns=remove_from_ratings)\n",
    "names = names.drop(columns=remove_from_names)\n",
    "\n",
    "\n",
    "print('Merging tables...')\n",
    "\n",
    "#merge relational tables\n",
    "\n",
    "crew_data = crew.copy()\n",
    "\n",
    "#merge crew data with names table to get respective names rather than nconst\n",
    "crew_data['nconst'] = crew_data['nconst'].str.split(', ')\n",
    "crew_data = crew_data.explode('nconst')\n",
    "crew_data = pd.merge(crew_data, names, on='nconst', how='left')\n",
    "crew_data = crew_data.pivot_table(\n",
    "    index=['tconst'],\n",
    "    columns=['category'],\n",
    "    values=['primaryName'],\n",
    "    aggfunc=lambda x: ', '.join(str(item) for item in x),\n",
    ").reset_index()\n",
    "\n",
    "#formaat and restructure columns\n",
    "crew_data.columns = [' '.join(col).strip() for col in crew_data.columns.values]\n",
    "crew_data.columns = ['tconst', 'actor', 'actress', 'archive_footage', 'archive_sound', 'cinematographer', 'composer', 'director', 'editor', 'producer', 'production_designer', 'self', 'writer']\n",
    "\n",
    "#merge datasets for one complete table\n",
    "crew_data = crew_data.drop(columns=['archive_footage','archive_sound','self', 'production_designer'])\n",
    "film_data = pd.merge(titles, ratings, on='tconst', how='left')\n",
    "film_data = pd.merge(film_data, crew_data, on='tconst', how='left')\n",
    "\n",
    "\n",
    "print('Further cleaning data...')\n",
    "\n",
    "#remove data-sparse films\n",
    "\n",
    "columns_check = ['director', 'cinematographer', 'editor', 'writer', 'composer', 'producer']\n",
    "film_data = film_data[film_data[columns_check].isna().sum(axis=1) < 2] #make smaller to decrease films?\n",
    "\n",
    "film_data = film_data.dropna(subset=['actor', 'actress'])\n",
    "film_data = film_data.dropna(subset=['runtimeMinutes'])\n",
    "film_data = film_data.dropna(subset=['averageRating'])\n",
    "film_data = film_data.dropna(subset=['genres'])\n",
    "\n",
    "film_data = film_data[film_data['runtimeMinutes'] != '\\\\N']\n",
    "film_data = film_data[film_data['startYear'] != '\\\\N']\n",
    "film_data = film_data[film_data['averageRating'] != '\\\\N']\n",
    "\n",
    "film_data['cast'] = film_data['actor'] + ', ' + film_data['actress']\n",
    "film_data.drop(['actor', 'actress'], axis=1, inplace=True)\n",
    "\n",
    "#add columns for plot and poster path\n",
    "film_data['plot'] = np.nan\n",
    "film_data['poster'] = np.nan\n",
    "\n",
    "print('Films: ' + str(len(film_data)))\n",
    "\n",
    "\n",
    "print('Fetching plot summaries and posters...')\n",
    "\n",
    "#get film plot and poster with tmdb api ~ >2hrs\n",
    "\n",
    "\n",
    "#call api/details for each film with multiprocessing and mutlithreading\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    manager = Manager()\n",
    "    shared_data = manager.Namespace()\n",
    "    agg_list = []\n",
    "\n",
    "    batch_size = 1000\n",
    "    sleep_time = 3\n",
    "\n",
    "    num_batches = (len(film_data) // batch_size) + 1\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(8) as process_executor:\n",
    "\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            start_index = i * batch_size\n",
    "            end_index = (i + 1) * batch_size\n",
    "            \n",
    "            shared_data.film_data = film_data.iloc[start_index:end_index]\n",
    "\n",
    "            future = process_executor.submit(doBatch, shared_data)\n",
    "\n",
    "            concurrent.futures.wait([future])\n",
    "\n",
    "            agg_list.append(shared_data.film_data)\n",
    "\n",
    "            print(f\"Batch {i+1}/{num_batches} completed\")\n",
    "                \n",
    "    film_data = pd.concat(agg_list, ignore_index=True)\n",
    "\n",
    "#remove films with no plot\n",
    "film_data = film_data.dropna(subset=['plot'])\n",
    "\n",
    "final_order = ['tconst','primaryTitle', 'plot', 'averageRating', 'genres', 'runtimeMinutes', 'startYear', 'cast', 'director', 'cinematographer', 'writer', 'producer', 'editor', 'composer', 'poster']\n",
    "film_data = film_data[final_order]\n",
    "\n",
    "print('Exporting to json...')\n",
    "\n",
    "\n",
    "#export film data to json\n",
    "\n",
    "#shuffle order\n",
    "film_data = film_data.sample(frac=1)\n",
    "result = film_data.to_json('webpage/films.json' ,orient=\"records\")\n",
    "\n",
    "print('Total films: ' + str(len(film_data)))\n",
    "\n",
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total films: ' + str(len(film_data)))\n",
    "\n",
    "film_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
